{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import sklearn.model_selection as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22068, 30)\n",
      "(22068,)\n",
      "(6790, 30)\n",
      "(6790,)\n",
      "(5094, 30)\n",
      "(5094,)\n"
     ]
    }
   ],
   "source": [
    "pc_train = pd.read_csv('pc_train.csv')\n",
    "pc_test = pd.read_csv('pc_test.csv')\n",
    "\n",
    "# I'm going to break off a validation dataset -- 1/5 of test\n",
    "# and 2/15 of train, adding up to 15% of total rows.\n",
    "\n",
    "# NOTE: (don't have time to change) I don't really need a validation\n",
    "# and test set using cross-validation.\n",
    "\n",
    "pc_train, valid1 = ms.train_test_split(pc_train, test_size = 2/15)\n",
    "pc_test, valid2 = ms.train_test_split(pc_test, test_size = 1/5)\n",
    "pc_valid = valid1.append(valid2)\n",
    "x_train = pc_train[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'winter', 'spring', 'summer', 'fall', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']]\n",
    "y_train = pc_train['target']\n",
    "x_test = pc_test[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'winter', 'spring', 'summer', 'fall', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']]\n",
    "y_test = pc_test['target']\n",
    "x_valid = pc_valid[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'winter', 'spring', 'summer', 'fall', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']]\n",
    "y_valid = pc_valid['target']\n",
    "\n",
    "# I'm going to break off a validation dataset -- 1/5 of test\n",
    "# and 2/15 of train, adding up to 15% of total rows.\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5858349483211118\n",
      "-0.5950358465524243\n",
      "-0.5654723540527746\n",
      "-0.5706264836450041\n"
     ]
    }
   ],
   "source": [
    "# Let's use mean absolute error and mean squared error as the \n",
    "# performance metrics. I will choose 5 folds to save time, \n",
    "# would normally choose 10.\n",
    "\n",
    "reg = LinearRegression()\n",
    "scores = cross_val_score(reg, x_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error')\n",
    "reg_score = sum(scores) / len(scores)\n",
    "print(reg_score)\n",
    "\n",
    "scores = cross_val_score(reg, x_train, y_train, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "reg_score = sum(scores) / len(scores)\n",
    "print(reg_score)\n",
    "\n",
    "reg_model = reg.fit(x_train, y_train)\n",
    "pickle.dump(reg_model, open('models/reg_model', 'wb'))\n",
    "\n",
    "# Well, that was the most basic score. Our future models\n",
    "# beat this value. Since this is the negative mean absolute\n",
    "# error, we are hunting for values closer to 0 than -0.585.\n",
    "\n",
    "gbr = GradientBoostingRegressor(loss = 'ls', n_estimators = 300)\n",
    "scores = cross_val_score(gbr, x_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error')\n",
    "gbr_score = sum(scores) / len(scores)\n",
    "print(gbr_score)\n",
    "\n",
    "scores = cross_val_score(gbr, x_train, y_train, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "gbr_score = sum(scores) / len(scores)\n",
    "print(gbr_score)\n",
    "\n",
    "gbr_model = gbr.fit(x_train, y_train)\n",
    "pickle.dump(gbr_model, open('models/gbr_model', 'wb'))\n",
    "\n",
    "# This is an improvement. The mean squared error decreased by 0.02.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6145246590167524\n",
      "-0.6405849252292314\n",
      "-0.568586235456171\n",
      "-0.5735189409605741\n"
     ]
    }
   ],
   "source": [
    "# This section can take quite a while to run.\n",
    "\n",
    "eln = ElasticNet(alpha = 0.2, max_iter = 10)\n",
    "scores = cross_val_score(eln, x_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error')\n",
    "eln_score = sum(scores) / len(scores)\n",
    "print(eln_score)\n",
    "\n",
    "eln = ElasticNet(alpha = 0.2, max_iter = 10)\n",
    "scores = cross_val_score(eln, x_train, y_train, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "eln_score = sum(scores) / len(scores)\n",
    "print(eln_score)\n",
    "\n",
    "eln_model = eln.fit(x_train, y_train)\n",
    "pickle.dump(eln_model, open('models/eln_model', 'wb'))\n",
    "\n",
    "# Well, this is no good. The mean absolute error decreases\n",
    "# as alpha approaches 0, but at 0, the elastic net becomes\n",
    "# a simple linear regression.\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 200)\n",
    "scores = cross_val_score(rfr, x_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error')\n",
    "rfr_score = sum(scores) / len(scores)\n",
    "print(rfr_score)\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 200)\n",
    "scores = cross_val_score(rfr, x_train, y_train, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "rfr_score = sum(scores) / len(scores)\n",
    "print(rfr_score)\n",
    "\n",
    "# The random forest model performs well but takes\n",
    "# a while to build the models. I think adding more\n",
    "# predictors would probably improve the results. I will\n",
    "# do that when I evaluate the models against the validation\n",
    "# results.\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 400)\n",
    "rfr_model = rfr.fit(x_train, y_train)\n",
    "pickle.dump(rfr_model, open('models/rfr_model', 'wb'))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create Neural Network Classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-158-5a5af783d79b>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-158-5a5af783d79b>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    self.conv1 = nn.Conv1d(in_channels=32, out_channels=, kernel_size=5)\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def outputSize(in_size, kernel_size, stride, padding):\n",
    "    output = int((in_size - kernel_size + 2*(padding)) / stride) + 1\n",
    "    return(output)\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.hidden1 = nn.Linear(30, 22)\n",
    "        self.hidden2 = nn.Linear(22, 3)\n",
    "        #self.hidden3 = nn.Linear(20, 16)\n",
    "        #self.hidden4 = nn.Linear(16, 12)\n",
    "        #self.hidden5 = nn.Linear(12, 8)\n",
    "        self.out = nn.Linear(3, 1)\n",
    "        #self.hidden = nn.Linear(178, 16)\n",
    "        #self.out = nn.Linear(16, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        x = nn.functional.relu(self.hidden1(x))\n",
    "        x = nn.functional.relu(self.hidden2(x))\n",
    "        #x = nn.functional.relu(self.hidden3(x))\n",
    "        #x = nn.functional.relu(self.hidden4(x))\n",
    "        #x = nn.functional.relu(self.hidden5(x))\n",
    "        x = sigmoid(self.out(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "# I didn't have enough time to configure and implement\n",
    "# the CNN and RNN models\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(6, 16, 5)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(in_features=16 * 41, out_features=256)\n",
    "        #self.fc2 = nn.Linear(128, 5)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 41)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        #x = self.fc2(x)\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        x = nn.functional.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRNN, self).__init__()\n",
    "        #self.rnn = nn.GRU(input_size=1, hidden_size=16, num_layers=2, batch_first=True, dropout=0.5)\n",
    "        #self.fc = nn.Linear(in_features=16, out_features=5)\n",
    "        \n",
    "        self.rnn1 = nn.GRU(input_size=1, hidden_size=32, num_layers=2, batch_first=True, dropout=0.5)\n",
    "        self.rnn2 = nn.GRU(input_size=32, hidden_size=128, num_layers=2, batch_first=True, dropout=0.5)\n",
    "        self.fc1 = nn.Linear(in_features=128, out_features=32)\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=16)\n",
    "        self.fc3 = nn.Linear(in_features=16, out_features=5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x, _ = self.rnn(x)\n",
    "        #x = torch.tanh(x[:, -1, :])\n",
    "        #x = self.fc(x)\n",
    "        \n",
    "        x, _ = self.rnn1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x, _ = self.rnn2(x)\n",
    "        x = torch.tanh(x[:, -1, :])\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  1.3810876607894897\n",
      "epoch:  5  loss:  0.8252854347229004\n",
      "epoch:  10  loss:  0.8142426609992981\n",
      "epoch:  15  loss:  0.8122402429580688\n",
      "epoch:  20  loss:  0.8117661476135254\n",
      "epoch:  24  loss:  0.8116387724876404\n"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE = 'MLP'  # Change this to 'MLP' or 'CNN'\n",
    "NUM_EPOCHS = 25\n",
    "BATCH_SIZE = 32\n",
    "USE_CUDA = False  \n",
    "NUM_WORKERS = 2  # Number of threads used by DataLoader\n",
    "\n",
    "target = Variable(torch.tensor(np.array(y_train)).float())\n",
    "MLP_data = Variable(torch.tensor(np.array(x_train)).float())\n",
    "MLP_data_test = Variable(torch.tensor(np.array(x_test)).float())\n",
    "\n",
    "model = MyMLP()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    y_pred = model(MLP_data)\n",
    "    loss = criterion(y_pred, target)\n",
    "    if epoch % 5 == 0:\n",
    "        print('epoch: ', epoch,' loss: ', loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print('epoch: ', epoch,' loss: ', loss.item())\n",
    "#pickle.dump(model, open('models/mlp.pth', 'wb'))\n",
    "\n",
    "# The MLP neural network doesn't seem to work well\n",
    "# in this case. I'd like to try others...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5842042271698018\n",
      "0.5658239520507302\n",
      "0.6130986277282414\n",
      "0.5684935151327679\n",
      "0.709200482834881\n",
      "0.7217957967215702\n"
     ]
    }
   ],
   "source": [
    "# Normally, if I had more time, I would have implemented\n",
    "# the CNN, RNN, and possibly other models. I would also\n",
    "# use cross-validation for the neural networks. There \n",
    "# is not a need for a test and validation dataset but\n",
    "# only a test set in cross-validation. I'm not going to\n",
    "# fix that here since there is no time. My last step is\n",
    "# to compare the results of the models using the test set.\n",
    "# There's a bunch more that I want to do but don't have time.\n",
    "\n",
    "reg_model\n",
    "gbr_model\n",
    "eln_model\n",
    "rfr_model\n",
    "model\n",
    "\n",
    "print(sklearn.metrics.mean_absolute_error(y_test, reg_model.predict(x_test)))\n",
    "print(sklearn.metrics.mean_absolute_error(y_test, gbr_model.predict(x_test)))\n",
    "print(sklearn.metrics.mean_absolute_error(y_test, eln_model.predict(x_test)))\n",
    "print(sklearn.metrics.mean_absolute_error(y_test, rfr_model.predict(x_test)))\n",
    "print(sklearn.metrics.mean_absolute_error(y_test, model(Variable(torch.tensor(np.array(x_test)).float())).detach()))\n",
    "\n",
    "mean_target = sum(y_test) / len(y_test)\n",
    "\n",
    "print(sum([abs(mean_target - a) for a in y_test]) / len(y_test))\n",
    "\n",
    "# It looks like on new data, it was a tie between the gbr\n",
    "# and rfr models. It looks like the ensemble methods win.\n",
    "\n",
    "# As I've said before, I ran out of time and was unable to\n",
    "# implement other models, like the RNN, which I believe\n",
    "# would probably improve the mean absolute error.\n",
    "\n",
    "# On any particular day, we expect the power company would be able to\n",
    "# predict the amount of active power used by this household\n",
    "# to within 0.565. That is significantly better than a pure guess,\n",
    "# which we would expect to be within about 0.72 of actual active power.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
